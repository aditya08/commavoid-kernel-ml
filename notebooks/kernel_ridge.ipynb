{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Kernel Ridge Regression\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Primal form:}~&\\Vert Ax - y \\Vert_2^2 + \\lambda \\Vert x \\Vert_2^2\\\\\n",
    "\\textbf{Closed-form solution:}~&x = \\left(A^TA + \\lambda I_n\\right)^{-1}A^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "From the identity $\\left(A^TA + \\lambda I_n\\right)^{-1}A^T = A^T\\left(AA^T + \\lambda I_m\\right)^{-1}$, we can obtain a closed-form solution with the alternate form:\n",
    "\\begin{align*}\n",
    "\\textbf{Alternate closed-form solution:}~x = A^T\\left(AA^T + \\lambda I_m\\right)^{-1}y\n",
    "\\end{align*}\n",
    "\n",
    "Let $A$ have $m$ rows and $n$ columns, such that each row is a data-point (or sample) and each column is a feature. In typical applications of kernel ridge regression, $m >> n$. This means that computing $\\alpha$ directly requires computing the $m \\times m$ matrix, $AA^T$. For very large $m$ this can be an expensive $m^2n$ operation and, in the extreme, we many not even have space to store $AA^T$.\n",
    "\n",
    "The Kernelized variant of ridge regression replaces the $m \\times n$ matrix $A$ with a high-dimensional feature space for each sample $\\Phi(A)$. $\\Phi(A)$ is computational-expensive to form, especially when the chosen kernel function has infinite-dimensional feature space (e.g. Radial Basis Function). However, the $m \\times m$ kernel matrix $K$ can be computed since the kernel functions are defined by $K_{i,j} = \\sum k(a_i, a_j)$, where $a_i$ is the $i$-th row of $A$ and $k(...)$ is a user-defined kernel function.\n",
    "\n",
    "One example of such a kernel function, is the polynomial kernel: $k(a_i, a_j) = (a_ia_j^T + r)^d$ s.t. $r \\geq 0$ and $d \\geq 1$. *When $r = 0$ and $d = 1$, we obtain a simple dot-product (or linear kernel).* In replacing the dot-products with a kernel function, we obtain a feature-space that can have very high dimensions (possibly infinite-dimensions). This means that computing the solution, $x$, is often very expensive. So instead, we will focus on obtaining the solution:\n",
    "\\begin{align*}\n",
    "\\bf\n",
    "\\text{Alternate solution:}~&\\alpha = \\left(\\frac{1}{\\lambda}K + I_m\\right)^{-1}y\\\\\n",
    "\\text{and}~&w = \\frac{1}{\\lambda}\\Phi(A)^T\\alpha\n",
    "\\end{align*}\n",
    "\n",
    "# 2. Subspace Iteration for Kernel Ridge Regression\n",
    "\n",
    "Note that the kernel matrix, $K$, is more expensive to compute than $AA^T$ (due to the kernel function), so we will instead compute the solution to $\\alpha$ iteratively using ***subspace iteration*** by selecting, and solving for a few samples (row of $A$) every iteration.\n",
    "\\begin{align*}\n",
    "\\text{Dual form:}~&\\Vert \\alpha - y \\Vert_2^2 + \\frac{1}{\\lambda} \\Vert \\Phi(A)^T\\alpha \\Vert_2^2\\\\\n",
    "\n",
    "\\bf\\text{Iterative update:}~&\\alpha_h = \\alpha_{h-1} + \\mathbb{I}_h^T {u_h}\\\\\n",
    "\\end{align*}\n",
    "The matrix $\\mathbb{I}_h$ has $b$ rows and $m$ columns such that rows of $\\mathbb{I}_h$ are sub-sampled rows of the $m$-dimensional identity matrix, $I_m$.\n",
    "\n",
    "By substituting the iterative update into the dual form, we obtain the following minimization problem:\n",
    "\\begin{align*}\n",
    "{\\arg\\min}~\\left\\Vert \\alpha_{h-1} + \\mathbb{I}_h^Tu_h - y \\right\\Vert_2^2 + \\frac{1}{\\lambda} \\left\\Vert \\Phi(A)^T\\alpha_{h-1} + \\Phi(A)^T\\mathbb{I}_h^T u_h\\right\\Vert_2^2\n",
    "\\end{align*}\n",
    "\n",
    "By taking the derivative w.r.t $u_h$ we can obtain the gradient for just the $b$ rows of $A$ that we are sampling at every iteration:\n",
    "\\begin{align*}\n",
    "0 &= \\mathbb{I}_h\\alpha_{h-1} + u_h - \\mathbb{I}_hy + \\frac{1}{\\lambda} \\mathbb{I}_h \\Phi(A)\\Phi(A)^T\\alpha_{h-1} + \\frac{1}{\\lambda}\\mathbb{I}_h\\Phi(A)\\Phi(A)^T\\mathbb{I}_h^T u_h\\\\\n",
    " \\textbf{Gradient w.r.t chosen samples: }u_h &= \\left(\\frac{1}{\\lambda} \\mathbb{I}_h\\Phi(A)\\Phi(A)^T\\mathbb{I}_h^T + I_b \\right)^{-1}\\left(\\mathbb{I}_h y - \\mathbb{I}_h \\alpha_{h-1} - \\frac{1}{\\lambda}\\mathbb{I}_h \\Phi(A)\\Phi(A)^T\\alpha_{h-1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that the term $\\mathbb{I}_h \\Phi(A)\\Phi(A)^T \\alpha_{h-1}$ does not permit the use of an auxiliary vector to represent $\\Phi(A)^T \\alpha_{h-1}$ due to the kernelization. This means that we will have to explicitly compute $\\mathbb{I}_h\\Phi(A)\\Phi(A)^T$ at every iteration. Additionally, the quantity $\\mathbb{I}_h\\Phi(A)\\Phi(A)^T\\mathbb{I}_h^T$ does not need to be computed at every iteration because we can simply select the relevant columns (namely the columns defined by right-multiplying by $\\mathbb{I}_h^T)$ from the quantity $\\mathbb{I}_h\\Phi(A)\\Phi(A)^T$.\n",
    "\n",
    "# 3. Communication-Avoiding Subspace Iteration for Kernel Ridge Regression\n",
    "\n",
    "In this section, we will derive the communication-avoiding variant of subspace iteration for solving the Kernel Ridge Regression problem. Lets start by re-stating the subspace iteration's update step:\n",
    "\\begin{align*}\n",
    "    u_{h+1} &= \\left(\\frac{1}{\\lambda} \\mathbb{I}_{h+1}\\Phi(A)\\Phi(A)^T\\mathbb{I}_{h+1}^T + I_b \\right)^{-1}\\left(\\mathbb{I}_{h+1} y - \\mathbb{I}_{h+1} \\alpha_{h} - \\frac{1}{\\lambda}\\mathbb{I}_{h+1} \\Phi(A)\\Phi(A)^T\\alpha_{h}\\right)\\\\\n",
    "    \\alpha_{h+1} &= \\alpha_{h} + \\mathbb{I}_{h+1}^T u_{h+1}\n",
    "\\end{align*}\n",
    "Given this two-line recurrence, we can write the next update for iteration $h+2$ as:\n",
    "\\begin{align}\n",
    "    u_{h+2} &= \\left(\\frac{1}{\\lambda} \\mathbb{I}_{h+2}\\Phi(A)\\Phi(A)^T\\mathbb{I}_{h+2}^T + I_b \\right)^{-1}\\left(\\mathbb{I}_{h+2} y - \\mathbb{I}_{h+2} \\alpha_{h+1} - \\frac{1}{\\lambda}\\mathbb{I}_{h+2} \\Phi(A)\\Phi(A)^T\\alpha_{h+1}\\right)\\tag{1}\\\\\n",
    "    \\alpha_{h+2} &= \\alpha_{h+1} + \\mathbb{I}_{h+2}^T u_{h+2}.\\tag{2}\n",
    "\\end{align}\n",
    "Notice that in the update step above, we can replace all instances of $\\alpha_{h}$ with $\\alpha_{h-1} + \\mathbb{I}_h^T u_h$. By doing this, we can write the update steps for $u_{h+1}$ and $\\alpha_{h+1}$ in terms of $\\alpha_{h-1}$ and $u_h$, instead of $\\alpha_h$:\n",
    "\n",
    "\\begin{align*}\n",
    "    u_{h+2} &= \\left(\\frac{1}{\\lambda} \\mathbb{I}_{h+2}\\Phi(A)\\Phi(A)^T\\mathbb{I}_{h+2}^T + I_b \\right)^{-1}\\left(\\mathbb{I}_{h+2} y \\color{blue}{- \\mathbb{I}_{h+2} \\alpha_{h} - \\frac{1}{\\lambda}\\mathbb{I}_{h+2} \\Phi(A)\\Phi(A)^T\\alpha_{h}} \\color{red}{- \\mathbb{I}_{h+2}\\mathbb{I}_{h+1}^T u_{h+1} - \\frac{1}{\\lambda}\\mathbb{I}_{h+2} \\Phi(A)\\Phi(A)^T\\mathbb{I}_{h+1}^T u_{h+1}}\\right)\\tag{3}\\\\\n",
    "    \\alpha_{h+2} &= \\alpha_{h} \\color{red}{+ \\mathbb{I}_{h+1}^Tu_{h+1} + \\mathbb{I}_{h+2}^T u_{h+2}}.\n",
    "\\end{align*}\n",
    "Comparing eq. (3) and (1), we can see that the terms in blue differ in that they depend on $\\alpha_{h}$ instead of $\\alpha_{h+1}$. The terms in red in eq. (3) are additional corrections which depend on the previous iteration's gradient, $u_{h+1}$, which also depend on $\\alpha_{h}$. Finally, we can update $\\alpha_{h+2}$ by summing the gradients $u_{h+1}$ and $u_{h+2}$.\n",
    "\n",
    "The example above performs a recurrence unrolling of two iterations (i.e. computing $\\alpha_{h+2}$ w.r.t $\\alpha_h$), but we can unroll for an arbitrary number of iterations. We will use the variable $s$ to represent the number of iterations we unroll the recurrences. Notice that we still compute all of the gradients, $\\{u_h, u_{h+1}, \\ldots, u_{h+s}\\}$, but only need $\\alpha_h$ in order to compute $\\alpha_{h+s}$. This means that we can defer updates to $\\alpha$ for $s$ iterations.\n",
    "\\begin{align*}\n",
    "u_{h + j} &= \\left(\\frac{1}{\\lambda} \\mathbb{I}_{h + j}\\Phi(A)\\Phi(A)^T\\mathbb{I}_{h + j}^T + I_b\\right)^{-1}\\left(\\mathbb{I}_{h+j} y \\color{blue}{- \\mathbb{I}_{h+j}\\alpha_{h} - \\frac{1}{\\lambda}\\mathbb{I}_{h+j}\\Phi(A)\\Phi(A)^T\\alpha_{h}} \\color{red}{- \\sum_{k = 1}^{j-1}\\mathbb{I}_{h + j}\\mathbb{I}_{h + k}^T u_{h + k} - \\sum_{k = 1}^{j-1} \\frac{1}{\\lambda}\\mathbb{I}_{h+j}\\Phi(A)\\Phi(A)^T\\mathbb{I}_{h + k}^T u_{h + k}}\\right)\\\\\n",
    "\\alpha_{h+j} &= \\alpha_{h} \\color{red}{+ \\sum_{k = 1}^{j} \\mathbb{I}_{h + k}^T u_{h + k}}.\\\\\n",
    "\\text{for } j &= \\{1, 2, \\ldots, s\\} \\text{ where $s$ is a free parameter.}\n",
    "\\end{align*}\n",
    "\n",
    "Notice that in the summation $\\sum_{k = 1}^{j-1} \\mathbb{I}_{h+k}\\mathbb{I}_{k + j}^T u_k$, the computation $\\mathbb{I}_{h+k}\\mathbb{I}_k^T$ is computing a $b \\times b$ identity-like matrix. This matrix is essentially computing the intersection between the sampled indices from iteration $h + j$ and iterations $h + k$ (where $k$ goes from $1$ to $j - 1$). This is crucial to ensure that gradient updates computed at iterations $h + k$ are incorporated into the iteration $h + j$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Kernel_Ridge_Regression(object):\n",
    "    def __init__(self, data, labels, kernel_params={'kernel_type': 'gaussian', 'sigma': 1e-1}):\n",
    "        self.kernel_params = kernel_params\n",
    "        self.nsamples, self.nfeatures = data.shape\n",
    "        if self.nsamples != len(labels):\n",
    "            raise ValueError('len(labels) != data.shape[1].')\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self._set_kernel_function(kernel_params['kernel_type'])\n",
    "        self.alpha = np.zeros(self.nsamples)\n",
    "\n",
    "    def _set_kernel_function(self, kernel_type):\n",
    "        kernel_type = kernel_type.lower()\n",
    "        if kernel_type == 'gaussian':\n",
    "             self.kernel_function = self._gaussian_kernel\n",
    "        elif kernel_type == 'polynomial':\n",
    "            self.kernel_function = self._polynomial_kernel\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel choice: {}\".format(kernel_type))\n",
    "    \n",
    "    def _gaussian_kernel(self, subsamples, samples):\n",
    "        kernel_matrix = pairwise_distances(subsamples, samples, metric='euclidean')**2\n",
    "        kernel_matrix = np.exp(-kernel_matrix/self.kernel_params['sigma']**2)\n",
    "        return kernel_matrix\n",
    "    \n",
    "    def _polynomial_kernel(self, subsamples, samples):\n",
    "        kernel_matrix = (subsamples.dot(samples.T) + self.kernel_params['shift']) ** self.kernel_params['dim']\n",
    "        return kernel_matrix\n",
    "    \n",
    "    def compute_objective(self):\n",
    "        obj_val = np.linalg.norm(self.alpha - self.labels)**2\n",
    "        obj_val += 1/self.C * (self.alpha.T.dot(self.kernel_function(self.data, self.data).dot(self.alpha)))\n",
    "        return obj_val\n",
    "    \n",
    "    def block_coordinate_descent(self, idxs):\n",
    "        blkrow_kernel_matrix = self.kernel_function(self.data[idxs, :], self.data)\n",
    "        residual = self.labels[idxs] - self.alpha[idxs] - (1/self.C)*blkrow_kernel_matrix.dot(self.alpha)\n",
    "        blkdiag_kernel_matrix = ((1/self.C) * blkrow_kernel_matrix[:, idxs]) + 1\n",
    "        gradient = np.linalg.solve(blkdiag_kernel_matrix, residual)\n",
    "        self.alpha[idxs] += gradient\n",
    "\n",
    "    def commavoid_block_coordinate_descent(self, idxs):\n",
    "        gradient = np.zeros(self.s*self.block_size)\n",
    "        blkrow_kernel_matrix = self.kernel_function(self.data[idxs, :], self.data)\n",
    "        residual = self.labels[idxs] - self.alpha[idxs] - (1/self.C)*blkrow_kernel_matrix.dot(self.alpha)\n",
    "        # compute 's' gradients.\n",
    "        blk_residual = residual[0:self.block_size]\n",
    "        blkdiag_kernel_matrix = ((1/self.C) * blkrow_kernel_matrix[0:self.block_size, idxs[0:self.block_size]]) + 1\n",
    "        gradient[0:self.block_size] = np.linalg.solve(blkdiag_kernel_matrix, blk_residual)\n",
    "        for i in range(1, self.s):\n",
    "            residual_correction = np.zeros(self.block_size)\n",
    "            blk_idxs = range(i*self.block_size, (i+1)*self.block_size)\n",
    "            blk_residual = residual[blk_idxs]\n",
    "            blkdiag_kernel_matrix = ((1/self.C) * blkrow_kernel_matrix[blk_idxs, idxs[blk_idxs]]) + 1\n",
    "            for j in range(i):\n",
    "                blk_gradient = gradient[j*self.block_size:(j+1)*self.block_size]\n",
    "                innerblk_idxs = idxs[j*self.block_size, (j+1)*self.block_size]\n",
    "                commonvals, idxof_outerblk, idxof_innerblk = np.intersect1d(blk_idxs, innerblk_idxs, assume_unique=True, return_indices=True)\n",
    "                residual_correction[idxof_outerblk] = blk_gradient[idxof_innerblk]\n",
    "                residual_correction += (1/self.C) * (blkrow_kernel_matrix[blk_idxs, j*self.block_size:(j+1)*self.block_size].dot(blk_gradient))\n",
    "            blk_residual -= residual_correction\n",
    "            gradient[blk_idxs] = np.linalg.solve(blkdiag_kernel_matrix, blk_residual)\n",
    "        self.alpha[idxs] += gradient\n",
    "\n",
    "    def train(self, epochs=100, optimizer='bcd', optimizer_params={'C': 1e-5, 'block_size': 1, 'shuffle': True}):\n",
    "        if optimizer.lower() == 'bcd':\n",
    "            optimizer = self.block_coordinate_descent\n",
    "        elif optimizer.lower() == 'cabcd':\n",
    "            optimizer = self.commavoid_block_coordinate_descent\n",
    "            self.s = optimizer_params['s']\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer choice: {}'.format(optimizer))\n",
    "        self.C = optimizer_params['C']\n",
    "        self.block_size = optimizer_params['block_size']\n",
    "        self.shuffle = optimizer_params['shuffle']\n",
    "        loss_history = []\n",
    "        for epoch in range(1,epochs+1):\n",
    "            if self.shuffle: \n",
    "                idxs = np.random.choice(self.nsamples, size=self.nsamples, replace=False)\n",
    "            else:\n",
    "                idxs = np.arange(self.nsamples)\n",
    "            nblocks = len(idxs)/self.block_size\n",
    "            for block in np.array_split(idxs, nblocks):\n",
    "                if len(block) < 1:\n",
    "                    break\n",
    "                optimizer(block)\n",
    "            loss_history.append(self.compute_objective())\n",
    "            epoch_strlen = len(str(epochs))\n",
    "            print('[{}] loss {:.16f}.'.format(str(epoch).zfill(epoch_strlen), loss_history[-1]))\n",
    "        plt.plot(loss_history)\n",
    "\n",
    "    def test(self):\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[001] loss 455588.3388916985713877.\n",
      "[002] loss 455588.0795302356709726.\n",
      "[003] loss 455587.8644772989791818.\n",
      "[004] loss 455587.6782239525928162.\n",
      "[005] loss 455587.5033736695186235.\n",
      "[006] loss 455587.3350848224945366.\n",
      "[007] loss 455587.1724890843615867.\n",
      "[008] loss 455587.0210916207870468.\n",
      "[009] loss 455586.8726343309972435.\n",
      "[010] loss 455586.7308336395653896.\n",
      "[011] loss 455586.5905959417577833.\n",
      "[012] loss 455586.4546944957692176.\n",
      "[013] loss 455586.3243158854311332.\n",
      "[014] loss 455586.1955792699009180.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ebaac01fd97c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/abalone'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mkrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKernel_Ridge_Regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mkrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-1a0762cb876f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, optimizer, optimizer_params)\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mepoch_strlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-1a0762cb876f>\u001b[0m in \u001b[0;36mblock_coordinate_descent\u001b[1;34m(self, idxs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mblock_coordinate_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mblkrow_kernel_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mblkrow_kernel_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mblkdiag_kernel_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mblkrow_kernel_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-1a0762cb876f>\u001b[0m in \u001b[0;36m_gaussian_kernel\u001b[1;34m(self, subsamples, samples)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_gaussian_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mkernel_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubsamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mkernel_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mkernel_matrix\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sigma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkernel_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1430\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1432\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mYY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \"\"\"\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    524\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m            \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m            indptr, indices, data)\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "(data, labels) = load_svmlight_file('../data/abalone')\n",
    "krr = Kernel_Ridge_Regression(data, labels)\n",
    "krr.train(epochs=100)"
   ]
  }
 ]
}