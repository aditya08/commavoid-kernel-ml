{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Kernel Ridge Regression\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Primal form:}~&\\Vert Ax - b \\Vert_2^2 + \\lambda \\Vert x \\Vert_2^2\\\\\n",
    "\\text{Closed-form solution:}~&x = \\left(A^TA + \\lambda I_n\\right)^{-1}A^Tb\\\\\n",
    "\\end{align*}\n",
    "\n",
    "From the identity $\\left(A^TA + \\lambda I_n\\right)^{-1}A^T = A^T\\left(AA^T + \\lambda I_m\\right)^{-1}$, we can obtain a closed-form solution with the alternate form:\n",
    "\\begin{align*}\n",
    "\\text{Alternate closed-form solution:}~x = A^T\\left(AA^T + \\lambda I_m\\right)^{-1}b\n",
    "\\end{align*}\n",
    "\n",
    "Let $A$ have $m$ rows and $n$ columns, such that each row is a data-point (or sample) and each column is a feature. In typical applications of kernel ridge regression, $m >> n$. This means that computing $\\alpha$ directly requires computing the $m \\times m$ matrix, $AA^T$. For very large $m$ this can be an expensive $m^2n$ operation and, in the extreme, we many not even have space to store $AA^T$.\n",
    "\n",
    "The Kernelized variant of ridge regression replaces the computation $AA^T$ with the $m \\times m$ kernel matrix, $K$.\n",
    "\n",
    "To alleviate this problem, we can instead compute $\\alpha$ iteratively using ***subspace iteration*** with a few data-points (rows) of $A$ every iteration.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Dual form:}~&\\Vert \\alpha - b \\Vert_2^2 + \\lambda \\Vert A^T\\alpha \\Vert_2^2\\\\\n",
    "\n",
    "\\text{Closed-form solution:}~&x = \\left(A^TA + \\lambda I_n\\right)^{-1}A^Tb\\\\\n",
    "\\end{align*}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}